# -*- coding: utf-8 -*-
"""Image Classification - David William Tanto, Oei.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V_LlsaQKOaYJLChFlcwLQuTz82oZqi3j

# Data Diri
Nama : David William Tanto, Oei

# Import Module
"""

import numpy as np 
import pandas as pd 
import random
import tensorflow as tf
import os,cv2
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report,confusion_matrix
import keras
from keras.models import Sequential
from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Input, Dropout

"""# Mount Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# Loading Data"""

paths = ['/content/drive/MyDrive/Indosat/kursus4/Classification/seg_train', 
         '/content/drive/MyDrive/Indosat/kursus4/Classification/seg_test']

labels = []
for folder in os.listdir(paths[0]):
    labels.append(folder)

print(labels)

dicc_labels={i:labels[i] for i in range(0,len(labels))}
dicc_labels

n = []
s = 64
_images = []
_labels = []
for j in range(0,2):
  for i,folder in enumerate(labels):
      try:
          for image in os.listdir(paths[j] +'/'+folder):
              img = os.path.join(paths[j]+'/'+folder+'/'+image)
              img = cv2.imread(img)
              img = cv2.resize(img,(s,s))
              _images.append(img)
              _labels.append(i)
      except:
          print("gagal")
  n.append(len(_images))

_images = np.asarray(_images)
_labels = np.asarray(_labels).astype('int64')
print("Ukuran gambar -> ",_images.shape)
print("Jumlah data per label -> ",np.bincount(_labels))

"""# Standard Scaler"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler() 
_images =_images.reshape((len(_images),-1))

scaler.fit(_images)
_images_scal = scaler.transform(_images)

"""# Modeling

## Splitting Data
"""

from keras.utils import to_categorical

X_train = _images_scal[0:n[0]]
X_test = _images_scal[n[0]:n[1]]

y_train = _labels[0:n[0]]
y_test = _labels[n[0]:n[1]]


X_train = X_train.reshape(len(X_train),s,s,3)
X_test = X_test.reshape(len(X_test),s,s,3)

print("Gambar Training-> ",X_train.shape, "labels train-> ",y_train.shape)
print("Jumlah per kategori (Training) -> ", np.bincount(y_train))
print("Gambar Testing -> ",X_test.shape, "labels train-> ",y_test.shape)
print("Jumlah per kategori (Testing) -> ", np.bincount(y_test))

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

"""## Build Model"""

model = keras.models.Sequential([
        keras.layers.Conv2D(5,kernel_size=(5,5),activation='relu',input_shape=(64,64,3)),
        keras.layers.AveragePooling2D (2,2),
        keras.layers.Conv2D(16,kernel_size=(5,5),activation='relu'),    
        keras.layers.MaxPool2D(2,2),
        keras.layers.Flatten() ,   
        keras.layers.Dense(512,activation='relu') ,            
        keras.layers.Dense(6,activation='softmax') ,    
        ])

"""## Compile Model"""

model.compile(optimizer="adam", 
              loss = "sparse_categorical_crossentropy",
              metrics = ["accuracy"]
)

"""## Model Summary"""

model.summary()

tf.keras.utils.plot_model(model, 
                          show_shapes = True, 
                          show_dtype = False, 
                          show_layer_names = True, 
                          expand_nested = True, 
                          show_layer_activations = True)

"""## Early Stopping"""

from keras.callbacks import EarlyStopping

es = EarlyStopping(monitor = 'val_loss',
                          min_delta = 0,
                          patience = 3,
                          verbose = 1,
                          restore_best_weights = True)

"""## Fitting Model"""

history = model.fit(X_train, y_train, 
                    validation_split = 0.2,
                    epochs = 10,
                    batch_size = 256,
                    callbacks = es,
                    verbose = 1)

"""## Visualisasi

### Training Accuracy
"""

plt.plot(history.history['accuracy'], label = "accuracy")
plt.plot(history.history['val_accuracy'], label = "val_accuracy")
plt.title('Train Accuracy vs Validation Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

"""### Training Model Loss"""

plt.plot(history.history['loss'], label = "loss")
plt.plot(history.history['val_loss'], label = "val_loss")
plt.title('Train loss vs Validation loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

print(dicc_labels)

pred = model.predict(X_test)

print(classification_report(
    np.argmax(y_test,axis=1),
    np.argmax(pred, axis=1)
))

"""# Save Model"""

save_path = 'mymodel/'
tf.saved_model.save(model, save_path)

converter = tf.lite.TFLiteConverter.from_saved_model("/content/mymodel")
tflite_model = converter.convert()
 
with tf.io.gfile.GFile('model_name.tflite', 'wb') as f:
    f.write(tflite_model)